{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Information_Extraction_IBRAHIM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOpiQMAPCnl+O/x8wCUTJiB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7OJBwGcjZfFo","executionInfo":{"status":"ok","timestamp":1619882750841,"user_tz":-330,"elapsed":6582,"user":{"displayName":"01_Mohammed Ibrahim_ Baig","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKvuN_1uPfPhzU7pPoczjPMo2k9UiXPb-mGcjGCmA=s64","userId":"01994611954354274238"}},"outputId":"febe48c5-d7ce-48f5-e414-9b916bbaab24"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import CFG\n","import matplotlib.pyplot as plt\n","import string\n","import re\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag\n","from tkinter import *\n","\n","  \n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag, ne_chunk\n","  \n","def named_entity_recognition(text):\n","    # tokenize the text\n","    word_tokens = word_tokenize(text)\n","  \n","    # part of speech tagging of words\n","    word_pos = pos_tag(word_tokens)\n","  \n","    # tree of word entities\n","    print(ne_chunk(word_pos))\n","  \n","text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n","named_entity_recognition(text)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  (ORGANIZATION GeeksforGeeks/NNP)\n","  so/RB\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Delhi/NNP)\n","  for/IN\n","  a/DT\n","  meetup/NN\n","  ./.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZDJPs01if0F","executionInfo":{"status":"ok","timestamp":1619882839830,"user_tz":-330,"elapsed":1913,"user":{"displayName":"01_Mohammed Ibrahim_ Baig","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKvuN_1uPfPhzU7pPoczjPMo2k9UiXPb-mGcjGCmA=s64","userId":"01994611954354274238"}},"outputId":"1db338c6-e285-48f5-c701-55d81f28f528"},"source":["from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","  \n","# convert text into word_tokens with their tags\n","def pos_tagging(text):\n","    word_tokens = word_tokenize(text)\n","    return pos_tag(word_tokens)\n","  \n","pos_tagging('You just gave me a scare')\n"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('You', 'PRP'),\n"," ('just', 'RB'),\n"," ('gave', 'VBD'),\n"," ('me', 'PRP'),\n"," ('a', 'DT'),\n"," ('scare', 'NN')]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0lFU1woip9s","executionInfo":{"status":"ok","timestamp":1619882867144,"user_tz":-330,"elapsed":1361,"user":{"displayName":"01_Mohammed Ibrahim_ Baig","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKvuN_1uPfPhzU7pPoczjPMo2k9UiXPb-mGcjGCmA=s64","userId":"01994611954354274238"}},"outputId":"3a6ff5ee-f17e-495f-853c-9aa08411cbd7"},"source":["# download the tagset \n","nltk.download('tagsets')\n","  \n","# extract information about the tag\n","nltk.help.upenn_tagset('NN')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n","NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvQH8VpFiwrr","executionInfo":{"status":"ok","timestamp":1619882892890,"user_tz":-330,"elapsed":978,"user":{"displayName":"01_Mohammed Ibrahim_ Baig","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKvuN_1uPfPhzU7pPoczjPMo2k9UiXPb-mGcjGCmA=s64","userId":"01994611954354274238"}},"outputId":"c2a5e566-526c-4e7a-831e-5880a0cb87fe"},"source":["from nltk.tree import *\n","from collections import defaultdict\n","from nltk.tokenize import word_tokenize \n","from nltk import pos_tag\n","  \n","# define chunking function with text and regular\n","# expression representing grammar as parameter\n","def chunking(text, grammar):\n","    word_tokens = word_tokenize(text)\n","  \n","    # label words with part of speech\n","    word_pos = pos_tag(word_tokens)\n","  \n","    # create a chunk parser using grammar\n","    chunkParser = nltk.RegexpParser(grammar)\n","  \n","    # test it on the list of word tokens with tagged pos\n","    tree = chunkParser.parse(word_pos)\n","\n","    for subtree in tree.subtrees():\n","        print(subtree)\n","    \n","      \n","sentence = 'the little yellow bird is flying in the sky'\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunking(sentence, grammar)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(S\n","  (NP the/DT little/JJ yellow/JJ bird/NN)\n","  is/VBZ\n","  flying/VBG\n","  in/IN\n","  (NP the/DT sky/NN))\n","(NP the/DT little/JJ yellow/JJ bird/NN)\n","(NP the/DT sky/NN)\n"],"name":"stdout"}]}]}